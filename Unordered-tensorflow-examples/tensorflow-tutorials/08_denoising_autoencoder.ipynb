{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Tutorial on how to create a denoising autoencoder w/ Tensorflow.\n",
    "\n",
    "Parag K. Mital, Jan 2016\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "from libs.utils import corrupt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def autoencoder(dimensions=[784, 512, 256, 64]):\n",
    "    \"\"\"Build a deep denoising autoencoder w/ tied weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dimensions : list, optional\n",
    "        The number of neurons for each layer of the autoencoder.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : Tensor\n",
    "        Input placeholder to the network\n",
    "    z : Tensor\n",
    "        Inner-most latent representation\n",
    "    y : Tensor\n",
    "        Output reconstruction of the input\n",
    "    cost : Tensor\n",
    "        Overall cost to use for training\n",
    "    \"\"\"\n",
    "    # input to the network\n",
    "    x = tf.placeholder(tf.float32, [None, dimensions[0]], name='x')\n",
    "\n",
    "    # Probability that we will corrupt input.\n",
    "    # This is the essence of the denoising autoencoder, and is pretty\n",
    "    # basic.  We'll feed forward a noisy input, allowing our network\n",
    "    # to generalize better, possibly, to occlusions of what we're\n",
    "    # really interested in.  But to measure accuracy, we'll still\n",
    "    # enforce a training signal which measures the original image's\n",
    "    # reconstruction cost.\n",
    "    #\n",
    "    # We'll change this to 1 during training\n",
    "    # but when we're ready for testing/production ready environments,\n",
    "    # we'll put it back to 0.\n",
    "    corrupt_prob = tf.placeholder(tf.float32, [1])\n",
    "    current_input = corrupt(x) * corrupt_prob + x * (1 - corrupt_prob)\n",
    "\n",
    "    # Build the encoder\n",
    "    encoder = []\n",
    "    for layer_i, n_output in enumerate(dimensions[1:]):\n",
    "        n_input = int(current_input.get_shape()[1])\n",
    "        W = tf.Variable(\n",
    "            tf.random_uniform([n_input, n_output],\n",
    "                              -1.0 / math.sqrt(n_input),\n",
    "                              1.0 / math.sqrt(n_input)))\n",
    "        b = tf.Variable(tf.zeros([n_output]))\n",
    "        encoder.append(W)\n",
    "        output = tf.nn.tanh(tf.matmul(current_input, W) + b)\n",
    "        current_input = output\n",
    "    # latent representation\n",
    "    z = current_input\n",
    "    encoder.reverse()\n",
    "    # Build the decoder using the same weights\n",
    "    for layer_i, n_output in enumerate(dimensions[:-1][::-1]):\n",
    "        W = tf.transpose(encoder[layer_i])\n",
    "        b = tf.Variable(tf.zeros([n_output]))\n",
    "        output = tf.nn.tanh(tf.matmul(current_input, W) + b)\n",
    "        current_input = output\n",
    "    # now have the reconstruction through the network\n",
    "    y = current_input\n",
    "    # cost function measures pixel-wise difference\n",
    "    cost = tf.sqrt(tf.reduce_mean(tf.square(y - x)))\n",
    "    return {'x': x, 'z': z, 'y': y,\n",
    "            'corrupt_prob': corrupt_prob,\n",
    "            'cost': cost}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "\n",
    "def test_mnist():\n",
    "    import tensorflow as tf\n",
    "    import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # %%\n",
    "    # load MNIST as before\n",
    "    mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "    mean_img = np.mean(mnist.train.images, axis=0)\n",
    "    ae = autoencoder(dimensions=[784, 256, 64])\n",
    "\n",
    "    # %%\n",
    "    learning_rate = 0.001\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(ae['cost'])\n",
    "\n",
    "    # %%\n",
    "    # We create a session to use the graph\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    # %%\n",
    "    # Fit all training data\n",
    "    batch_size = 50\n",
    "    n_epochs = 10\n",
    "    for epoch_i in range(n_epochs):\n",
    "        for batch_i in range(mnist.train.num_examples // batch_size):\n",
    "            batch_xs, _ = mnist.train.next_batch(batch_size)\n",
    "            train = np.array([img - mean_img for img in batch_xs])\n",
    "            sess.run(optimizer, feed_dict={\n",
    "                ae['x']: train, ae['corrupt_prob']: [1.0]})\n",
    "        print(epoch_i, sess.run(ae['cost'], feed_dict={\n",
    "            ae['x']: train, ae['corrupt_prob']: [1.0]}))\n",
    "\n",
    "    # %%\n",
    "    # Plot example reconstructions\n",
    "    n_examples = 15\n",
    "    test_xs, _ = mnist.test.next_batch(n_examples)\n",
    "    test_xs_norm = np.array([img - mean_img for img in test_xs])\n",
    "    recon = sess.run(ae['y'], feed_dict={\n",
    "        ae['x']: test_xs_norm, ae['corrupt_prob']: [0.0]})\n",
    "    fig, axs = plt.subplots(2, n_examples, figsize=(10, 2))\n",
    "    for example_i in range(n_examples):\n",
    "        axs[0][example_i].imshow(\n",
    "            np.reshape(test_xs[example_i, :], (28, 28)))\n",
    "        axs[1][example_i].imshow(\n",
    "            np.reshape([recon[example_i, :] + mean_img], (28, 28)))\n",
    "    fig.show()\n",
    "    plt.draw()\n",
    "    plt.waitforbuttonpress()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_mnist()"
   ]
  }
 ],
 "metadata": {
  "language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
