{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Simple tutorial using code from the TensorFlow example for Regression.\n",
    "\n",
    "Parag K. Mital, Jan. 2016\"\"\"\n",
    "# pip3 install --upgrade\n",
    "# https://storage.googleapis.com/tensorflow/mac/tensorflow-0.6.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import tensorflow as tf\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# get the classic mnist dataset\n",
    "# one-hot means a sparse vector for every observation where only\n",
    "# the class label is 1, and every other class is 0.\n",
    "# more info here:\n",
    "# https://www.tensorflow.org/versions/0.6.0/tutorials/mnist/download/index.html#dataset-object\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# mnist is now a DataSet with accessors for:\n",
    "# 'train', 'test', and 'validation'.\n",
    "# within each, we can access:\n",
    "# images, labels, and num_examples\n",
    "print(mnist.train.num_examples,\n",
    "      mnist.test.num_examples,\n",
    "      mnist.validation.num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% the images are stored as:\n",
    "# n_observations x n_features tensor (n-dim array)\n",
    "# the labels are stored as n_observations x n_labels,\n",
    "# where each observation is a one-hot vector.\n",
    "print(mnist.train.images.shape, mnist.train.labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% the range of the values of the images is from 0-1\n",
    "print(np.min(mnist.train.images), np.max(mnist.train.images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% we can visualize any one of the images by reshaping it to a 28x28 image\n",
    "plt.imshow(np.reshape(mnist.train.images[100, :], (28, 28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% We can create a container for an input image using tensorflow's graph:\n",
    "# We allow the first dimension to be None, since this will eventually\n",
    "# represent our mini-batches, or how many images we feed into a network\n",
    "# at a time during training/validation/testing.\n",
    "# The second dimension is the number of features that the image has.\n",
    "n_input = 784\n",
    "n_output = 10\n",
    "net_input = tf.placeholder(tf.float32, [None, n_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% We can write a simple regression (y = W*x + b) as:\n",
    "W = tf.Variable(tf.zeros([n_input, n_output]))\n",
    "b = tf.Variable(tf.zeros([n_output]))\n",
    "net_output = tf.nn.softmax(tf.matmul(net_input, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% We'll create a placeholder for the true output of the network\n",
    "y_true = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% And then write our loss function:\n",
    "cross_entropy = -tf.reduce_sum(y_true * tf.log(net_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% This would equate each label in our one-hot vector between the\n",
    "# prediction and actual using the argmax as the predicted label\n",
    "correct_prediction = tf.equal(\n",
    "    tf.argmax(net_output, 1), tf.argmax(y_true, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% And now we can look at the mean of our network's correct guesses\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% We can tell the tensorflow graph to train w/ gradient descent using\n",
    "# our loss function and an input learning rate\n",
    "optimizer = tf.train.GradientDescentOptimizer(\n",
    "    0.01).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% We now create a new session to actually perform the initialization the\n",
    "# variables:\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Now actually do some training:\n",
    "batch_size = 100\n",
    "n_epochs = 10\n",
    "for epoch_i in range(n_epochs):\n",
    "    for batch_i in range(mnist.train.num_examples // batch_size):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        sess.run(optimizer, feed_dict={\n",
    "            net_input: batch_xs,\n",
    "            y_true: batch_ys\n",
    "        })\n",
    "    print(sess.run(accuracy,\n",
    "                   feed_dict={\n",
    "                       net_input: mnist.validation.images,\n",
    "                       y_true: mnist.validation.labels\n",
    "                   }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Print final test accuracy:\n",
    "print(sess.run(accuracy,\n",
    "               feed_dict={\n",
    "                   net_input: mnist.test.images,\n",
    "                   y_true: mnist.test.labels\n",
    "               }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "# We could do the same thing w/ Keras like so:\n",
    "from keras.models import Sequential\n",
    "model = Sequential()\n",
    "\n",
    "from keras.layers.core import Dense, Activation\n",
    "model.add(Dense(output_dim=10, input_dim=784, init='zero'))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "    optimizer=SGD(lr=learning_rate))\n",
    "\n",
    "model.fit(mnist.train.images, mnist.train.labels, nb_epoch=n_epochs,\n",
    "          batch_size=batch_size, show_accuracy=True)\n",
    "\n",
    "objective_score = model.evaluate(mnist.test.images, mnist.test.labels,\n",
    "                                 batch_size=100, show_accuracy=True)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
